#202411_task2_離職預測二分類_XGBoost_RandomizedSearchCV 調參
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import uniform, randint

# Step 1: Load data
train_df = pd.read_excel('/AISD-interviewAssignment/train.xlsx')
test_df = pd.read_excel('/AISD-interviewAssignment/test.xlsx')

# Step 2: Preprocess data
# Separate features and target variable from training data
X = train_df.drop(columns=['Exited'])  # Exclude target variable
y = train_df['Exited']

# Handle categorical variables with One-Hot Encoding for both train and test sets
categorical_cols = X.select_dtypes(include=['object']).columns

# Apply One-Hot Encoding to both train and test datasets
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Ensure the test set has the same columns as the train set
test_df = test_df.reindex(columns=X.columns, fill_value=0)

# Step 3: Train-test split for validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define XGBoost model and parameters for RandomizedSearchCV
xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')

# Define parameter distribution for random search
param_dist = {
    'n_estimators': randint(50, 300),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(3, 10),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'scale_pos_weight': randint(1, 5)
}

# Use RandomizedSearchCV for parameter tuning
random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=50, cv=3, scoring='f1', verbose=1, random_state=42)
random_search.fit(X_train, y_train)

# Best parameters and model
print("Best Parameters:", random_search.best_params_)
best_model = random_search.best_estimator_

# Step 5: Model evaluation on validation set with best model
y_pred_val = best_model.predict(X_val)
f1 = f1_score(y_val, y_pred_val)
conf_matrix = confusion_matrix(y_val, y_pred_val)

# Step 6: Predict on test set and output CSV
y_pred_test = best_model.predict(test_df)
output = pd.DataFrame({'Predicted Exited': y_pred_test})
output.to_csv('predictions.csv', index=False)

# Step 7: Evaluation metrics and visualization
print("F1 Score:", f1)
print("Confusion Matrix:\n", conf_matrix)

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.savefig('XGBoost_confusion_matrix.png')
plt.show()

# Plot ROC Curve
y_pred_proba = best_model.predict_proba(X_val)[:, 1]
fpr, tpr, _ = roc_curve(y_val, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.savefig('XGBoost_roc_curve.png')
plt.show()
